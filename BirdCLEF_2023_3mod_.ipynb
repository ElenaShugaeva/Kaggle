{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqWlKsKwAt9i"
   },
   "source": [
    "# Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWzhAhakIceM"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile, Path\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import wandb \n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6psY3frY0_YB",
    "outputId": "045adccd-c7f2-4a3d-f879-01488d419426"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkQyUprfPoDb"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    n_size = 15                # 5 10  15\n",
    "    sr = 32000\n",
    "    batch_size = 256             #512  r64\n",
    "    learning_rate= 0.0001      \n",
    "    architecture= \"CNN\"      #['CNN','resnet34', 'M5']\n",
    "    epochs= 10\n",
    "    seed = 15\n",
    "    api = \n",
    "    project = 'birdCLEF'\n",
    "    entity = \n",
    "    wandb = False\n",
    "    if device == \"cuda\":\n",
    "        num_workers = 1\n",
    "        pin_memory = True\n",
    "    else:\n",
    "        num_workers = 0\n",
    "        pin_memory = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X--lTvk2zKPN"
   },
   "outputs": [],
   "source": [
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IhjSNpW18KG"
   },
   "outputs": [],
   "source": [
    "seed = CFG.seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random_state= 15\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True \n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K40Wf0pBP9o5",
    "outputId": "92196024-c7b4-48f8-bd0e-834242a40b1e"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BUiK3cLLL7I",
    "outputId": "9e28dd6b-724c-4506-9e8c-9bee9f729129"
   },
   "outputs": [],
   "source": [
    "with ZipFile('birdclef-2023.zip') as myzip:\n",
    "        print(len(myzip.namelist()))\n",
    "        print(myzip.namelist()[:10])\n",
    "        csv_list = []\n",
    "        for name in myzip.namelist():\n",
    "            if name.endswith('.csv'):\n",
    "                csv_list.append(name)\n",
    "        print(csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zi7Cqr62zE-B",
    "outputId": "6a39214a-e446-4e93-c6e1-34e70fcd3f99"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with ZipFile('birdclef-2023.zip') as myzip:\n",
    "    data_train = myzip.open('train_metadata.csv')\n",
    "    data_sample = myzip.open('sample_submission.csv')\n",
    "    data_ebird = myzip.open('eBird_Taxonomy_v2021.csv')\n",
    "    \n",
    "df_train = pd.read_csv(data_train)\n",
    "df_sample = pd.read_csv(data_sample)\n",
    "df_ebird = pd.read_csv(data_ebird)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESWZsTHGA1bZ"
   },
   "source": [
    "# Step 2: Explore the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34n8yjYnSHTJ"
   },
   "source": [
    "## review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8DfYZUuQz8-h",
    "outputId": "03040125-70c4-44b2-c873-3824c5f8437f"
   },
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_eGzvYZP_5CM",
    "outputId": "8ae48e8b-0de4-4533-de8a-59a165da7942"
   },
   "outputs": [],
   "source": [
    "df_ebird.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "TH-ss98y0V6X",
    "outputId": "3c44a1b1-4fa5-438e-ba72-ebb8f46188bd"
   },
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "safa_Pfr3g6o",
    "outputId": "80226f99-a49b-4867-d65d-ddd98687fe89"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with ZipFile('/content/drive/MyDrive/birdclef-2023.zip') as myzip:\n",
    "    data_1 = myzip.open('train_audio/abethr1/XC128013.ogg')\n",
    "    data_2 = myzip.open('train_audio/abhori1/XC120250.ogg')\n",
    "au_abe, sr_abe = librosa.load(data_1)\n",
    "au_abh, sr_abh = librosa.load(data_2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ASBrFUCw4rXU",
    "outputId": "15121426-7301-4296-be49-037a7eba4d75"
   },
   "outputs": [],
   "source": [
    "Audio(data=au_abe, rate=sr_abe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "2RIUwFd82juf",
    "outputId": "e84cc5c3-cb9f-4f38-fcb1-d07083fa50be"
   },
   "outputs": [],
   "source": [
    "Audio(data=au_abh, rate=sr_abh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eTiHfXWT8Rgv",
    "outputId": "987409d9-14c8-4396-bfba-bed9ad9de9e3"
   },
   "outputs": [],
   "source": [
    "# abethr1\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(au_abe)\n",
    "plt.show()\n",
    "# abhori1\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(au_abh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72ITBuTOBGyK",
    "outputId": "6475c498-bb0d-4331-820c-4c621dc2e809"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with ZipFile('/content/drive/MyDrive/birdclef-2023.zip') as myzip:\n",
    "      data_test = myzip.open('test_soundscapes/soundscape_29201.ogg')\n",
    "au_test, sr_test = librosa.load(data_test)\n",
    "Audio(data=au_test, rate= sr_test)\n",
    "print(sr_test)\n",
    "print(len(au_test)/sr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "PCNg1cdKOs7X",
    "outputId": "93f71d7c-13c3-4e6f-80cb-0151c6496000"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(au_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhTDUcpmQYQd"
   },
   "source": [
    "## waveform_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qnpo_N38Cxa"
   },
   "outputs": [],
   "source": [
    "def get_train_data_(filename):\n",
    "    with ZipFile('birdclef-2023.zip') as myzip:\n",
    "        return myzip.open(f'train_audio/{filename}')\n",
    "\n",
    "def low_count_classes(series):\n",
    "      return series.value_counts()[series.value_counts() < 2].index.tolist()\n",
    "\n",
    "def splitting(df, split_size):\n",
    "      df['train'] = df['primary_label'].isin(low_count_classes(df['primary_label']))\n",
    "      df_without_single =df[~df['train']]\n",
    "      max_df, min_df = train_test_split(df_without_single, test_size=split_size, stratify=df_without_single['primary_label'], random_state=random_state)\n",
    "      max_df = pd.concat([max_df, df[df['train']]], axis=0).reset_index(drop=True)\n",
    "      max_df.drop('train', axis=1, inplace=True)\n",
    "      min_df.drop('train', axis=1, inplace=True)\n",
    "      return max_df, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwgWnbbXOiZt"
   },
   "outputs": [],
   "source": [
    "df1, df2 = splitting(df_train, 0.5)              #in case lack of memory\n",
    "train_df,val_df = splitting(df1, 0.3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwwGNRK8RKqw"
   },
   "outputs": [],
   "source": [
    "def label_list(df, encoder):                     #first option\n",
    "      list_filename =df['filename'].tolist()\n",
    "      le = encoder\n",
    "      labels = le.fit_transform (df['primary_label'])\n",
    "      zipped = zip(labels, list_filename)\n",
    "      return list(zipped), set(labels)\n",
    "zipped_list_train, classes = label_list(train_df, LabelEncoder())  \n",
    "zipped_list_val, _ = label_list(val_df, LabelEncoder()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXedPMS9TUHP"
   },
   "outputs": [],
   "source": [
    "def label_lists(df):                               #sec option\n",
    "      list_filename =df['filename'].tolist()\n",
    "      labels = df['primary_label']\n",
    "      zipped = zip(labels, list_filename)\n",
    "      return list(zipped), set(labels)\n",
    "list_train, classes = label_lists(train_df)  \n",
    "list_val, _ = label_lists(val_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUdQU32vO8Ef",
    "outputId": "3ad4974a-dd2f-47a2-a910-b0d4cdfed5fd"
   },
   "outputs": [],
   "source": [
    "train_set =[]                                          #Mel here help decrease allocated memory\n",
    "for label, item in tqdm (list_train):\n",
    "    waveform, sample_rate = torchaudio.load(get_train_data_(item))\n",
    "    if waveform.shape[1] > CFG.n_size*CFG.sr:\n",
    "        waveform = torch.narrow(waveform, 1,0,CFG.n_size*CFG.sr)               \n",
    "    else:\n",
    "        delta = CFG.n_size*CFG.sr- waveform.shape[1]\n",
    "        waveform = F.pad(waveform,(0,delta), \"constant\", 0) \n",
    "    waveform = torchaudio.transforms.MelSpectrogram(n_fft = 2504)(waveform)\n",
    "    train_set.append([waveform, sample_rate, label])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "VOjggfFsO8Eg",
    "outputId": "a5e590ae-c09d-428f-edc6-11732db6a8f8"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(waveform.log2()[0,:,:].numpy(), cmap='viridis')  \n",
    "plt.title(f'{label}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8w-_HwQMO8Eg",
    "outputId": "b4767fcd-ebe4-4dd2-87a0-c448be0476e6"
   },
   "outputs": [],
   "source": [
    "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7N4MnDuV1Wa",
    "outputId": "b13d5646-3efa-4f3f-84c0-cdb25be0cd7e"
   },
   "outputs": [],
   "source": [
    "val_set =[]                                        \n",
    "for label, item in tqdm (list_val):\n",
    "    waveform, sample_rate = torchaudio.load(get_train_data_(item))\n",
    "    if waveform.shape[1] > CFG.n_size*CFG.sr:\n",
    "        waveform = torch.narrow(waveform, 1,0,CFG.n_size*CFG.sr)               \n",
    "    else:\n",
    "        delta = CFG.n_size*CFG.sr- waveform.shape[1]\n",
    "        waveform = F.pad(waveform,(0,delta), \"constant\", 0) \n",
    "    waveform = torchaudio.transforms.MelSpectrogram(n_fft = 2504)(waveform)    \n",
    "    val_set.append([waveform, sample_rate, label])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vrs9GAJ9kN4q"
   },
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(labels.index(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BP2KxtckdwM"
   },
   "outputs": [],
   "source": [
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3oG9yuiKPyk",
    "outputId": "05331051-07ed-4fd6-d251-6f62e487a8da"
   },
   "outputs": [],
   "source": [
    "def get_test_data_(filename):\n",
    "    with ZipFile('birdclef-2023.zip') as myzip:\n",
    "        return myzip.open(f'test_soundscapes/{filename}')\n",
    "waveform, sample_rate = torchaudio.load(get_test_data_('soundscape_29201.ogg'))\n",
    "if waveform.shape[1] > CFG.n_size*CFG.sr:\n",
    "    waveform = torch.narrow(waveform, 1,0,CFG.n_size*CFG.sr)\n",
    "else:\n",
    "    delta = CFG.n_size*CFG.sr- waveform.shape[1]\n",
    "    waveform = F.pad(waveform,(0,delta), \"constant\", 0)\n",
    "mel_spectrogram_test = torchaudio.transforms.MelSpectrogram(sample_rate= sample_rate, n_fft = 2504)(waveform)  \n",
    "mel_spectrogram_test.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkS3AZjYS4sn"
   },
   "outputs": [],
   "source": [
    "transform = torch.nn.Sequential(\n",
    "     nn.Flatten(2,3) # for batch, check for tensor\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0LIqmrRO8Eh"
   },
   "source": [
    "# Step 3: Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKD_eeTja8pi"
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_oxfaQClXAC"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label,       \n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, _, label in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "  \n",
    "    targets = torch.stack(targets)\n",
    "    tensors = torch.stack(tensors)\n",
    "\n",
    "    return tensors, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExKwf2qTncU8"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers= CFG.num_workers,\n",
    "    pin_memory=CFG.pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=CFG.num_workers,\n",
    "    pin_memory=CFG.pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ5lWOq6U-12"
   },
   "outputs": [],
   "source": [
    "#----- without function, first opt\n",
    "train_loader = torch.utils.data.DataLoader(wavet, batch_size=CFG.batch_size,shuffle=True,  num_workers=CFG.num_workers,pin_memory=CFG.pin_memory)\n",
    "val_loader = torch.utils.data.DataLoader(wavev, batch_size=CFG.batch_size,shuffle=True,  num_workers=CFG.num_workers,pin_memory=CFG.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWcntGW1v7K-",
    "outputId": "7464849b-acc0-4613-a415-984d0bd4e10a"
   },
   "outputs": [],
   "source": [
    "for batch, (X, Y) in enumerate(train_loader):    #pls check carefully\n",
    "    print(batch,X.shape,transform(X).shape, Y.shape)\n",
    "    print(batch,type(X), type(Y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtoaKe8Ap-K2"
   },
   "source": [
    "## M5 pytorch tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5g_gKwIp9gi",
    "outputId": "b31a6797-4482-4d22-97c1-16eb227f2c2c"
   },
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=226, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "        # self.fl1 = nn.Flatten(2,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fl1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "transformed = mel_spectrogram_test\n",
    "model = M5(n_input=transformed.shape[0], n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zv4_gUCnvd4v",
    "outputId": "ee273d61-dd11-4a1f-d47b-b6553776550c"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=( 1, 128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WybgDYMcuw02"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPsIaxjIvD9Z"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3U8bVaSvRqr"
   },
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrWWzwqXRXID",
    "outputId": "caf12b64-f8ad-4cd6-c3e6-77d91d3fbf19"
   },
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 50\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "transform = transform.to(device)\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train(model, epoch, log_interval)\n",
    "        test(model, epoch)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "dwNQ0z6kntBu",
    "outputId": "a1510fc0-0d74-4c34-9d5c-67b9166ad4db"
   },
   "outputs": [],
   "source": [
    "# Let's plot the training loss versus the number of iteration.\n",
    "plt.plot(losses);\n",
    "plt.title(\"training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cz--OBYsoM_B",
    "outputId": "98bb1726-e2b0-4f3a-cd28-7a38088d9bed"
   },
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    # tensor = transform(tensor) # flatten different for batch / tensor\n",
    "    tensor = tensor.reshape(1, -1)\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "\n",
    "waveform, sample_rate, utterance, *_ = train_set[73]\n",
    "\n",
    "print(f\"Expected: {utterance}. Predicted: {predict(waveform)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zd3w7yH-o7QK"
   },
   "outputs": [],
   "source": [
    "prediction =[]\n",
    "with torch.no_grad():\n",
    "    tensor = mel_spectrogram_test.to(device)\n",
    "    tensor = tensor.reshape(1, -1)\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    \n",
    "    pred = tensor.cpu().numpy()\n",
    "    argmax = np.argmax(pred)\n",
    "    probs = np.exp(pred[0]).tolist() \n",
    "df = pd.DataFrame(probs, columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "4DMHW-jPqJvi",
    "outputId": "b628b304-73c7-47bd-ff66-4d90a9ca5bc7"
   },
   "outputs": [],
   "source": [
    "index_to_label(argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4sDrFx7t-e1",
    "outputId": "e75cacb3-a1f3-472b-d339-9e97e4ecc76a"
   },
   "outputs": [],
   "source": [
    "argmax = np.argmax(pred)\n",
    "print(argmax, np.exp(pred[0][0][argmax]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample1 = pd.read_csv('df_M5_5.csv')\n",
    "df_sample2 = df_sample1.drop(['Unnamed: 0'], axis = 1)\n",
    "df_sample2.loc[ df_sample2.row_id == \"soundscape_29201_15\", labels] = df.values\n",
    "df_sample2.to_csv('df_M5_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.read_csv('df_M5_15.csv')\n",
    "submission = w.drop(['Unnamed: 0'], axis = 1)\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated()/1024**2)\n",
    "!nvidia-smi\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBOMWtELNVE-"
   },
   "source": [
    "## CNN MS_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nInm49RWssP"
   },
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(53824, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 264)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(x,dim=1)     \n",
    "\n",
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Narrow(nn.Module):\n",
    "    def __init__(self, dim, start, length):\n",
    "        super(Narrow, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.start = start\n",
    "        self.length = length\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.narrow(x, self.dim, self.start, self.length)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = nn.Sequential(Narrow(3,0,128)) # for batch\n",
    "for batch, (X, Y) in enumerate(train_loader):\n",
    "    print(batch,X.shape,transform(X).shape, Y.shape)\n",
    "    print(batch,type(X), type(Y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5e2I7FyO8Ei"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=( 1, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae5QcntyXEHB"
   },
   "outputs": [],
   "source": [
    "cost = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = CFG.learning_rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scaler = GradScaler()               #Automatic Mixed Precision (AMP)\n",
    "NUM_ACCUMULATION_STEPS =2         #gradient accumulation (GA)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    if CFG.wandb:\n",
    "            os.environ[\"WANDB_API_KEY\"] = CFG.api\n",
    "            wandb.init(project=CFG.project, name=CFG.architecture, entity=CFG.entity, reinit=True, config=class2dict(CFG))\n",
    "        \n",
    "    model.train()\n",
    "    correct = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "            Y= Y.type(torch.LongTensor)  \n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():            \n",
    "                pred = model(X)\n",
    "                loss = cost(pred, Y)\n",
    "                \n",
    "            loss = loss / NUM_ACCUMULATION_STEPS        \n",
    "                \n",
    "            scaler.scale(loss).backward()       \n",
    "            \n",
    "            if ((batch + 1) % NUM_ACCUMULATION_STEPS == 0) or (batch + 1 == len(dataloader)):     \n",
    "                scaler.step(optimizer)              \n",
    "                scaler.update()                     \n",
    "\n",
    "            if batch % 1000 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "                tqdm.write(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "    if CFG.wandb:\n",
    "        wandb.log({'train_loss': loss/size,\n",
    "                  'train_accuracy': correct / size})   \n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            X = transform(X)\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    tqdm.write(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')\n",
    "    if CFG.wandb:\n",
    "        wandb.log({'test_loss': test_loss,\n",
    "                   'test_accuracy': correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rZ9eIwH8okA"
   },
   "outputs": [],
   "source": [
    "for t in range(CFG.epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_loader, model, cost, optimizer)\n",
    "    test(val_loader, model)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K44cy5dNEy9h"
   },
   "outputs": [],
   "source": [
    "prediction =[]\n",
    "with torch.no_grad():\n",
    "    X_t = mel_spectrogram_test.unsqueeze(0).to(device)\n",
    "    pred = model(X_t).cpu().numpy()\n",
    "    argmax = np.argmax(pred)\n",
    "    preds = pred[0].tolist() \n",
    "df = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDpdj_WFOu7b"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(preds)\n",
    "classes_inv = le.inverse_transform(np.array(list(classes)))\n",
    "df_sample.loc[df_sample.row_id == \"soundscape_29201_5\" , classes_inv] = df.T\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRpa98SKSn3u"
   },
   "outputs": [],
   "source": [
    "df_sample.to_csv('df_sample_CNN_5.csv')\n",
    "files.download('df_sample_CNN_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bETsyzi1Ngom"
   },
   "source": [
    "## pretrained resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gz0Gd7xM1HC"
   },
   "outputs": [],
   "source": [
    "wavet =[]       #as option\n",
    "\n",
    "for label, item in tqdm (zipped_list_train):\n",
    "    waveform, sample_rate = torchaudio.load(get_train_data_(item))\n",
    "    if waveform.shape[1] > CFG.n_size*CFG.sr:\n",
    "        waveform = torch.narrow(waveform, 1,0,CFG.n_size*CFG.sr)                                      \n",
    "    else:\n",
    "        delta = CFG.n_size*CFG.sr- waveform.shape[1]\n",
    "        waveform = F.pad(waveform,(0,delta), \"constant\", 0)                                            \n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate= sample_rate, n_mels = 256,  n_fft = 2048)(waveform) \n",
    "  #1D-->3D\n",
    "    mel_spectrogram_ = mel_spectrogram.expand(3,*mel_spectrogram.shape[1:]) \n",
    "\n",
    "    mel_transforms = transforms.Compose([\n",
    "        transforms.CenterCrop(224),        \n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])(mel_spectrogram_)\n",
    "\n",
    "    wavet.append([mel_transforms, label])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Jh-BmjDwMiW"
   },
   "outputs": [],
   "source": [
    "wavev =[]\n",
    "\n",
    "for label, item in tqdm (zipped_list_val):\n",
    "    waveform, sample_rate = torchaudio.load(get_train_data_(item))\n",
    "   \n",
    "    if waveform.shape[1] > CFG.n_size*CFG.sr:\n",
    "        waveform = torch.narrow(waveform, 1,0,CFG.n_size*CFG.sr)              \n",
    "    else:\n",
    "        delta = CFG.n_size*CFG.sr- waveform.shape[1]\n",
    "        waveform = F.pad(waveform,(0,delta), \"constant\", 0)                    \n",
    "        \n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate= sample_rate, n_mels = 256,  n_fft = 2048)(waveform) \n",
    "    #1D-->3D\n",
    "    mel_spectrogram_ = mel_spectrogram.expand(3,*mel_spectrogram.shape[1:]) \n",
    "\n",
    "    mel_transforms = transforms.Compose([\n",
    "        transforms.CenterCrop(224),        \n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])(mel_spectrogram_)\n",
    "\n",
    "    wavev.append([mel_transforms, label])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usR4KEp6UNri"
   },
   "outputs": [],
   "source": [
    "def get_test_data_3(filename):\n",
    "    with ZipFile('birdclef-2023.zip') as myzip:\n",
    "        return myzip.open(f'test_soundscapes/{filename}')\n",
    "    waveform, sample_rate = torchaudio.load(get_test_data_3('soundscape_29201.ogg'))\n",
    "if waveform.shape[1] > CFG.n_size*CFG.sr:\n",
    "    waveform = torch.narrow(waveform, 1,0,CFG.n_size*CFG.sr).to(device)\n",
    "else:\n",
    "    delta = CFG.n_size*CFG.sr- waveform.shape[1]\n",
    "    waveform = F.pad(waveform,(0,delta), \"constant\", 0).to(device)\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate= sample_rate, n_fft = 2048)(waveform.cpu())  \n",
    "mel_spectrogram_test = mel_spectrogram.expand(3,*mel_spectrogram.shape[1:])\n",
    "mel_transforms_test = transforms.Compose([\n",
    "        transforms.CenterCrop(224),        \n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])(mel_spectrogram_test) \n",
    "mel_transforms_test.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBTN0KpqSbwH"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34\n",
    "rnet = torchvision.models.resnet34(weights='ResNet34_Weights.DEFAULT')\n",
    "summary(rnet, input_size=( 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xRkh8nXFfLP"
   },
   "outputs": [],
   "source": [
    "rnet = models.resnet34(weights='ResNet34_Weights.DEFAULT')\n",
    "for param in rnet.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = rnet.fc.in_features\n",
    "rnet.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 264),\n",
    "    nn.ReLU(),\n",
    "    nn.Softmax(dim=1))\n",
    "\n",
    "model = rnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhFgIpJQO8Ek"
   },
   "outputs": [],
   "source": [
    "for t in range(CFG.epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_loader, model, cost, optimizer)\n",
    "    test(val_loader, model)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDKaNu5UhEhi"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  \n",
    "    X_t = mel_transforms_test.unsqueeze(0).to(device)\n",
    "    pred = model(X_t).cpu().numpy()\n",
    "preds = pred[0].tolist()   \n",
    "argmax = np.argmax(pred)\n",
    "print(argmax, pred[0][argmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEHDIet-iKDe"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(preds)\n",
    "classes_inv = le.inverse_transform(np.array(list(classes)))\n",
    "df_sample.loc[df_sample.row_id == \"soundscape_29201_5\" , classes_inv] = df.T\n",
    "df_sample.to_csv('df_sample_resnet34_5.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "34n8yjYnSHTJ"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "eis_kernel",
   "language": "python",
   "name": "eis_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
